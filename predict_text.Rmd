---
title: "Text Prediction"
author: "Jonathan Owen"
date: "March 14, 2016"
output: 
  html_document:
    theme: simplex
    
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```  

```{r libraries, message = FALSE} 
library(tm)
library(filehash)
library(data.table)
library(dplyr)
library(ggplot2)
library(htmlTable)
```  

```{r get_data}
if (!file.exists("Coursera-SwiftKey.zip")) {
  file_url <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
  download.file(file_url, "Coursera-SwiftKey.zip")
  }
if (!file.exists("final/en_US")) {unzip("Coursera-SwiftKey.zip")}
``` 

```{r process_files, cache = TRUE, message = FALSE}
# read and write documents as vectors, returns number of lines
processFiles <- function(x){
                  inFile <- paste0("final/en_US/en_US.", x, ".txt")
                  inCon <- file(inFile, open = "rb")
                  vText <- readLines(inCon, skipNul = TRUE)
                  close(inCon)
                  vText <- gsub("\032", "", vText)
                  outFile <- paste0("en_US/", x, ".txt")
                  outCon <- file(outFile, open = "wt")
                  writeLines(vText, outCon)
                  close(outCon)
                  length(unlist(strsplit(vText, split = " ")))
}

docType <- c("blogs", "news", "twitter")
nWords <- sapply(docType, processFiles)
```  

```{r create_corpus, cache = TRUE, message = FALSE, warning = FALSE}
english <- PCorpus(DirSource("en_US/"), 
                  readerControl = list(reader = readPlain, 
                                       language = "en_US",
                                       load = TRUE),
                  dbControl = list(useDb = TRUE,
                                   dbName = "dbEnglish",
                                   dbtype = "DB1"))
```  

```{r basic_data, cache = TRUE, message = FALSE}
getLines <- function(x){length(english[[x]]$content)}
nLines <- sapply(c(1:3), getLines)
dfEnglish <- data.frame("document" = docType, "line count" = nLines, 
                        "word count" = nWords, "word per line" = round(nWords/nLines, digits = 1))
```  

```{r clean_corpus, cache = TRUE, message = FALSE, warning = FALSE}
toSpace <- content_transformer(function(x, pattern){return (gsub(pattern, " ", x))})
english <- tm_map(english, content_transformer(tolower))
english <- tm_map(english, toSpace, "[^abcdefghijklmnopqrstuvwxyz']")
# english <- tm_map(english, removeWords, stopwords("english"))
english <- tm_map(english, stripWhitespace)
```  

```{r stem_corpus, cache = TRUE}
# english <- tm_map(english, stemDocument, language = "english")
```  

```{r write_corpus, message = FALSE, warning = FALSE}
writeCorpus(english, "clean/")
```  

```{r get_clean, cache = TRUE, message = FALSE}
# read clean corpus documents as vectors
readClean <- function(x){
  inFile <- paste0("clean/", x, ".txt.txt")
  inCon <- file(inFile, open = "rb")
  vClean <- readLines(inCon, skipNul = TRUE)
  close(inCon)
  vClean
}

lstClean <- sapply(docType, readClean)
lstClean <- strsplit(unlist(lstClean), split = " ")
wordBlogs <- unlist(lstClean[1:nLines[1]])
wordNews <- unlist(lstClean[(1+nLines[1]):(nLines[1]+nLines[2])])
wordTwitter <- unlist(lstClean[(1+nLines[1]+nLines[2]):(nLines[1]+nLines[2]+nLines[3])])
wordBlogs <- data.table("term" = wordBlogs)
wordNews <- data.table("term" = wordNews)
wordTwitter <- data.table("term" = wordTwitter)
``` 

```{r term_frequency, cache = TRUE, message = FALSE}
freqBlogs <- wordBlogs %>%
             group_by(term) %>%
             summarise(frequency = n())
freqBlogs <- arrange(freqBlogs, desc(frequency))
freqBlogs <- freqBlogs[-46,]
freqBlogs$count <- freqBlogs$frequency
freqBlogs$frequency <- freqBlogs$count/sum(freqBlogs$count)

freqNews <- wordNews %>%
            group_by(term) %>%
            summarise(frequency = n())
freqNews <- arrange(freqNews, desc(frequency))
freqNews <- freqNews[-17,]
freqNews$count <- freqNews$frequency
freqNews$frequency <- freqNews$count/sum(freqNews$count)

freqTwitter <- wordTwitter %>%
               group_by(term) %>%
               summarise(frequency = n())
freqTwitter <- arrange(freqTwitter, desc(frequency))
freqTwitter <- freqTwitter[-23,]
freqTwitter$count <- freqTwitter$frequency
freqTwitter$frequency <- freqTwitter$count/sum(freqTwitter$count)
```  


```{r filter_profanity, cache = TRUE}
replaceProfanity <-content_transformer(function(x, pattern){return (gsub(pattern, "xxxxx", x))})
profanity <- readLines("profanity.txt")
profanity <- paste0(profanity, collapse = "|")
```  

<br></br>  
 
  
#### Summary
A corpus of text from blogs, news, and Tweets has been downloaded, imported, 
cleaned, and summarized as term frequencies.  The data contained 4269678 lines, 
101832226 words.  
<br></br> 

#### Context
This is a milestone report on getting, cleaning, and exploratory analysis of data 
for the Johns Hopkins University Data Science Specialization capstone project 
offered through Coursera.  The overall goal of the project is to develop and 
deploy an application that will predict the next word that follows a 
sequence of words.
<br></br>  

#### Data
Data are text extracts from blogs, new articles, and Tweets collected at 
[www.corpora.heliohost.org](www.corpora.heliohost.org). 
A [README](http://www.corpora.heliohost.org/aboutcorpus.html) is also 
available. The dataset for this project were download from [Coursera](https://eventing.coursera.org/api/redirectStrict/KsmFaVOZnKH-sR1CQPWnUy3lgUgZ26he929raUvnq78Cau1TPljxrZIhX5ucbL0gRe3hgUnzyEBKzydra5XVmQ.ncPRkYkDfbbsy1D3zDbZ2g.zBFKFvOtjW4ly9cP8ZBglyM3jdkEMJsDNnfL-aO1H1W9rGoa-l7n21FZK2dhyRaLD3qYfO1oNZjIM8IfPwTcNZtKMUW_9lbcNY4ZsuKzCTplV2bOCpKIGgLdM76iEfmjXPPo2L_veZx8I9hFP-aXmTPCy2fDtss2Zk7Uzs35h22u_m6KBytwRYY47KZuyAxbR2X1bVN92ipKCj6U9m3PjuYPYEUoAexScgDWMOByhi5a1JYt3bx-0Ijb_TVeyB93M1BSZ7i5gKyctzcGnMzkgS6rfEpRqU_HS0UsBsQipYDVajqxDpJXZvPv9J_F6CFBo-z4bg0-6lrI5JT9U58jAjEIrlM7k48GLPW_F-0YV_VecKmH7ffwVF2RhzP4yWXk1xyzjhRCdlXr6eaxwLtzVWt7TbDC1CKJToy1Nc48xZ4).  From the data, 3 files in the final/en_US folder 
(English language) were used: blogs.txt, news.txt, and twitter.txt 
A list of censored words monitored by Google was obtained from [Ryan Lewis' github repository](https://gist.github.com/ryanlewis/a37739d710ccdb4b406d/archive/0fbd315eb2900bb736609ea894b9bde8217b991a.zip).
This was used to create profanity filter to be applied to the data in later phases of the project.  
<br></br> 

#### Data Processing
After downloading and unzipping the data.  The files were read line by line. 
Several lines in news.txt included an "end of file" character.  After reading 
the data as binary, the characters were removed and the files were written to 
a folder en_US. These were then imported into a permanent corpus using the *tm* 
package. Initial properties before any cleaning or pre-processing are summarized 
below.  

**Basic properties of the blogs.txt, news.txt, and twitter.txt documents**
```{r basic_table}
htmlTable(dfEnglish[2:4], header = c( "..# of lines..", " ..# of words.. ", "words per line"))
```  

Although the dataset is large (approximately 600 MB, and 4.5 million lines), no 
sampling and only minimal cleaning was done to allow as close as possible to the 
raw data to be used for the exploratory data analysis. All text was converted to 
lower case, all characters apart from the letters a to z and the apostrophe were 
removed, and extra white space was stripped from the documents.  No stopwords 
were removed and no stemming has been done at this point.  

<br></br> 

#### Term Frequencies
The size of the dataset prevented the *TermDocumentMatrix* function from the *tm* 
package from being used to count terms (words) and calculate frequencies. Instead 
the lines of each document were split into words and summarized using the 
*dplyr* package.

```{r plot_blogs, echo=FALSE}
pBlogs <- ggplot(freqBlogs[1:50], aes(row_number(frequency), frequency)) +
          geom_bar(stat="identity") +
  ggtitle("50 most frequently occurring words in blogs text") +
          scale_x_continuous(name = "term", breaks = c(1:50), 
                             labels = freqBlogs$term[50:1]) +
          theme(axis.ticks.x = element_blank(),
                axis.text.x  = element_text(angle = 90, hjust = 1))
pBlogs
```

```{r plot_news, echo=FALSE}
pNews <- ggplot(freqNews[1:50], aes(row_number(frequency), frequency)) +
          geom_bar(stat="identity") +
  ggtitle("50 most frequently occurring words in news text") +
          scale_x_continuous(name = "term", breaks = c(1:50), 
                             labels = freqNews$term[50:1]) +
          theme(axis.ticks.x = element_blank(),
                axis.text.x  = element_text(angle = 90, hjust = 1))
pNews
```  

```{r plot_twitter, echo=FALSE}
pTwitter <- ggplot(freqTwitter[1:50], aes(row_number(frequency), frequency)) +
          geom_bar(stat="identity") +
  ggtitle("50 most frequently occurring words in Twitter text") +
          scale_x_continuous(name = "term", breaks = c(1:50), 
                             labels = freqTwitter$term[50:1]) +
          theme(axis.ticks.x = element_blank(),
                axis.text.x  = element_text(angle = 90, hjust = 1))
pTwitter
```  

<br></br> 

#### Next steps
The current work only goes as far as the preliminary data analysis. The following 
tasks will be needed in order to begin developing a prediction application.  
  
  
+   Investigate frequencies of word pairs (bi-grams) and triplets (tri-grams)  

+   Sample data to train prediction algorithm  

+   Measure performance (resource usage, calculation time) and prediction accuracy for different algorithm tuning parameters.  



#Appendix  
<br></br>  

#### Libraries
```{r, ref.label = "libraries", echo = TRUE, eval = FALSE}
```  

#### Get data  
```{r, ref.label = "get_data", echo = TRUE, eval = FALSE}
```  

#### Process files  
```{r, ref.label = "process_files", echo = TRUE, eval = FALSE}
```  

#### Create corpus 
```{r, ref.label = "create_corpus", echo = TRUE, eval = FALSE}
```   

#### Get basic properties of data 
```{r, ref.label = "basic_data", echo = TRUE, eval = FALSE}
```   

#### Clean corpus 
```{r, ref.label = "clean_corpus", echo = TRUE, eval = FALSE}
```   

#### Write cleaned corpus 
```{r, ref.label = "write_corpus", echo = TRUE, eval = FALSE}
```   

#### Get cleaned corpus files
```{r, ref.label = "get_clean", echo = TRUE, eval = FALSE}
```   

#### Calculate term frequencies 
```{r, ref.label = "term_frequency", echo = TRUE, eval = FALSE}
```   

#### Print table and plot data
```{r, ref.label = "basic_table", echo = TRUE, eval = FALSE}
```   

```{r, ref.label = "plot_blogs", echo = TRUE, eval = FALSE}
```   

```{r, ref.label = "plot_news", echo = TRUE, eval = FALSE}
```   

```{r, ref.label = "plot_twitter", echo = TRUE, eval = FALSE}
``` 
